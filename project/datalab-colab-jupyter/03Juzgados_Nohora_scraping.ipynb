{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"03Juzgados_Nohora_scraping.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"i9E-0Bit_OK1"},"source":["## Objective of this Notebook"]},{"cell_type":"markdown","metadata":{"id":"PkzPzhye_UzW"},"source":["\n","**Business Context.** Como estrategia de Alianza CAOBA se quiere entender del universo de empresas (Privadas y Publicas) cuales son las que tienen mayor inclinacion analitica para ser contactadas y asi enfocar mejor los esfuerzo de la estrategia comercial"]},{"cell_type":"markdown","metadata":{"id":"lHb9WNAw_V9W"},"source":["**Business Problem.** El lider de innovacion quiere resolver la siguiente pregunta: **\"Cuales son las empresas que se deben contactar dado variables asociadas con innovacion en analtica internamente en las entidades (Privadas y Publicas)\"**. \n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ruwsWVqY_WIO"},"source":["**Analytical Context.** Las fuentes a usar son:\n","\n","*   Archivos de transacciones del banco X\n","\n","Se quiere al final de este ejercicio tener una solucion capaz de:\n","\n","1.   Recibir las fuentes organizadas\n","2.   Generar una vista minable\n","3.   Como resultado 1 se tenga => una analisis descriptivo de las transacciones\n","4.   Como resultado 1 se tenga => una analisis de segmentacion de los gastos\n","\n","**Anotaciones:**\n","\n","-Para llenar"]},{"cell_type":"markdown","metadata":{"id":"kAeYyOJ4Nyt4"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"0G84nkX_-1_Z"},"source":["# Setting Colab, and Getting data from path in MyDrive"]},{"cell_type":"code","metadata":{"id":"CCAZaoLs-w3V","executionInfo":{"status":"ok","timestamp":1616122497248,"user_tz":420,"elapsed":407,"user":{"displayName":"Kamy Nz","photoUrl":"","userId":"05812003528127999158"}}},"source":["# This cell will prompt you to connect this notebook with your googles account.\n","#from google.colab import drive\n","#drive.mount(\"/content/gdrive\",force_remount=True)\n","#root_dir = \"/content/gdrive/My Drive/\""],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"O40w_7TuXZOz","executionInfo":{"status":"ok","timestamp":1616122497514,"user_tz":420,"elapsed":663,"user":{"displayName":"Kamy Nz","photoUrl":"","userId":"05812003528127999158"}}},"source":["from google.colab import auth\n","auth.authenticate_user()\n"," \n","import gspread\n","from oauth2client.client import GoogleCredentials\n","gc = gspread.authorize(GoogleCredentials.get_application_default())"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"mvDEWmrVQHkY","executionInfo":{"status":"ok","timestamp":1616122497515,"user_tz":420,"elapsed":655,"user":{"displayName":"Kamy Nz","photoUrl":"","userId":"05812003528127999158"}}},"source":["#from google.colab import auth\n","#auth.authenticate_user()\n","\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","from gspread_dataframe import get_as_dataframe, set_with_dataframe"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"OVMTto-jSNan","executionInfo":{"status":"ok","timestamp":1616122497516,"user_tz":420,"elapsed":630,"user":{"displayName":"Kamy Nz","photoUrl":"","userId":"05812003528127999158"}}},"source":["# Authenticate and create the PyDrive client.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YjQ_PKKI-55t","executionInfo":{"status":"ok","timestamp":1616122497517,"user_tz":420,"elapsed":616,"user":{"displayName":"Kamy Nz","photoUrl":"","userId":"05812003528127999158"}},"outputId":"7b1243c1-b032-4764-c9d2-b738f5f57db4"},"source":["import os\n","base_dir = \"/content/gdrive/My Drive/Colab Notebooks/My_projects/00-Scraping_RamaJudicial/\"\n","print(base_dir)"],"execution_count":28,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/Colab Notebooks/My_projects/00-Scraping_RamaJudicial/\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RLQs2x1U9XCB"},"source":["## Mini-tutorial of Web Scraping and Web Crawling tomado de [Tutorials](https://www.tutorialspoint.com/python_web_scraping/python_web_scraping_introduction.htm)\n","\n","\n","### Origin of Web Scraping\n","\n","*   The origin of web scraping is screen scrapping, which was used to integrate non-web based applications or native windows applications. Originally screen scraping was used prior to the wide use of World Wide Web (WWW), but it could not scale up WWW expanded. This made it necessary to automate the approach of screen scraping and the technique called ‘Web Scraping’ came into existence\n","\n","### Web Crawling v/s Web Scraping\n","\n","\n","*   The terms Web Crawling and Scraping are often used interchangeably as the basic concept of them is to extract data. However, they are different from each other. We can understand the basic difference from their definitions. Web crawling is basically used to index the information on the page using bots aka crawlers. It is also called indexing. On the hand, web scraping is an automated way of extracting the information using bots aka scrapers. It is also called data extraction.\n","\n","\n","### Working of a Web Scraper\n","\n","Web scraper may be defined as a software or script used to download the contents of multiple web pages and extracting data from it.\n"]},{"cell_type":"markdown","metadata":{"id":"Lv6sbOmIX5eC"},"source":["## Important LINKs:\n","\n","https://www.tutorialspoint.com/python_web_scraping/index.htm\n","\n","https://linuxhint.com/get_filea_metadata_exif_tool/\n","\n","https://dzone.com/articles/extracting-pdf-metadata-and-text-with-python\n","\n","https://towardsdatascience.com/how-to-extract-data-from-pdf-forms-using-python-10b5e5f26f70\n","\n","https://exifmeta.com/"]},{"cell_type":"markdown","metadata":{"id":"X61StaDfX2vt"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"4d14_YUFVQdv"},"source":["## Suppor LINK (Do instrucctions in following links):\n","\n","1. [Best Python Modules to Parse - 2021 acording to catswhocode](https://catswhocode.com/python-modules-list/)\n","2. [Exercise with Beatiful Soup](https://zetcode.com/python/beautifulsoup/)\n","3. [Good Documentation in Python](https://realpython.com/edocumenting-python-code/)\n","4. [Best way to loop over DataFrame](https://medium.com/swlh/how-to-efficiently-loop-through-pandas-dataframe-660e4660125d)\n","5. http://www.alirazabhayani.com/2018/01/python-html-to-text-for-sending-sms-sms.html\n","6. https://docs.python.org/3/howto/unicode.html\n","7. https://medium.com/swlh/most-common-string-methods-in-python-a7c9094f3fd3\n","8. https://realpython.com/python-strings/\n","9. https://medium.com/@bfortuner/python-multithreading-vs-multiprocessing-73072ce5600b\n","10. https://towardsdatascience.com/google-colab-import-and-export-datasets-eccf801e2971"]},{"cell_type":"markdown","metadata":{"id":"BQ9GlQrjZnU_"},"source":["## FUNCIONES"]},{"cell_type":"code","metadata":{"id":"NLuG9bkrZqze","executionInfo":{"status":"ok","timestamp":1616123658745,"user_tz":420,"elapsed":1075,"user":{"displayName":"Kamy Nz","photoUrl":"","userId":"05812003528127999158"}}},"source":["# Funciones\n","def f3aux_normalizing_href(hrefinput):\n","\n","  href = hrefinput\n","\n","  if(href.startswith(\"/documents/\")):\n","    #print(hrefinput+\" To normalize point1\\n\")\n","    href_normalize = \"https://www.ramajudicial.gov.co\"+str(href)\n","  else:\n","    href_normalize = str(href)\n","  \n","  #print(href_normalize+\" To normalize point2\\n\")\n","  return(href_normalize)"],"execution_count":58,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lv0Q3goZZq72","executionInfo":{"status":"ok","timestamp":1616123658920,"user_tz":420,"elapsed":966,"user":{"displayName":"Kamy Nz","photoUrl":"","userId":"05812003528127999158"}}},"source":["# Loading sheets from GoogleSheets in GoogleDrive\n","def f0_load_sheets_from_source(source_url,namesheet1,namesheet2):\n","  \"\"\"Gets the positive result of a change in status of law processes\n","\n","    Parameters\n","    ----------\n","    source_url : url address\n","        Link to GoogleSheets to parse as db_juzgados\n","    json_name : str\n","        Name of the JSON to be created\n","    destination : str\n","        PATH in which the created HTML is going to be saved\n","\n","    Returns\n","    -------\n","    df_act_juzgados : str\n","        A message saying that process passed without issues\n","\n","    df_act_subjuzgados : str\n","        A message saying that process passed without issues\n","\n","    prints : Different prints\n","        Differents prints of the processing for loading sheets\n","  \"\"\"\n","  db_juzgados = gc.open_by_url(source_url)\n","  main_sheet = db_juzgados.worksheet(namesheet1)\n","  juzgados_sheet = db_juzgados.worksheet(namesheet2)\n","  df_act_juzgados = get_as_dataframe(main_sheet)\n","  df_act_juzgados = df_act_juzgados.loc[:,~df_act_juzgados.columns.str.match(\"Unnamed\")]\n","\n","  df_act_subjuzgados = get_as_dataframe(juzgados_sheet)\n","  df_act_subjuzgados = df_act_subjuzgados.loc[:,~df_act_subjuzgados.columns.str.match(\"Unnamed\")]\n","\n","  #dict_df_query = {}\n","  #for tab in query_list:\n","  #  dict_df_query[tab] = pd.read_excel(io.BytesIO(io_query),sheet_name=tab)\n","  #return(dict_df_query)\n","\n","  print(df_act_juzgados.dtypes)\n","  print(\"\\n\")\n","  print(df_act_juzgados.columns)\n","  print(\"\\n\")\n","  print(df_act_juzgados.shape)\n","  print(\"\\n\")\n","  print(type(df_act_juzgados))\n","\n","  print(df_act_subjuzgados.dtypes)\n","  print(\"\\n\")\n","  print(df_act_subjuzgados.columns)\n","  print(\"\\n\")\n","  print(df_act_subjuzgados.shape)\n","  print(\"\\n\")\n","  print(type(df_act_subjuzgados))\n","\n","  return(df_act_juzgados,df_act_subjuzgados)\n","\n","def f4_generating_merge_subjuzgados_report(dfbyreport,user_name,destination):\n","  \n","  \"\"\"Gets the positive result of a change in status of law processes\n","\n","    Parameters\n","    ----------\n","    dfbyreport: dataframe \n","        Dataframe of all scrape juzgados sort by Fecha Actualizacion\n","    user_name : str\n","        Name of client\n","    destination : str\n","        PATH of folder in which file should be written in\n","\n","    Returns\n","    -------\n","    df_act_juzgados : str\n","        A message saying that process passed without issues\n","\n","    df_act_subjuzgados : str\n","        A message saying that process passed without issues\n","\n","    prints : Different prints\n","        Differents prints of the processing for loading sheets\n","  \"\"\"\n","\n","  # Import Drive API and authenticate.\n","  from google.colab import drive\n","  from datetime import date\n","  from datetime import datetime\n","\n","  drive.mount(\"/content/drive\",force_remount=True)\n","  root_dir = \"/content/drive/My Drive/\"\n","\n","  date_execution_report = str(date.today())\n","  csv_name = \"reporte_\"+user_name+\"_\"+date_execution_report\n","\n","  try:\n","    # Write the DataFrame to CSV file.\n","    #with open(destination+csv_name+\".xlsx\", 'w') as f:\n","    #  dfbyreport.to_excel(f)\n","    dfbyreport.to_excel(destination+csv_name+\".xlsx\",sheet_name = user_name+\"_\"+date_execution_report,index=False)\n","  except Exception as e:\n","    print(e)\n","\n","  #df.to_csv('data.csv')\n","  #!cp data.csv \"drive/My Drive/\"\n","def structuring_sending_options():\n","  pass\n","\n"],"execution_count":59,"outputs":[]},{"cell_type":"code","metadata":{"id":"qKXnPOwlZ8_J","executionInfo":{"status":"ok","timestamp":1616123659085,"user_tz":420,"elapsed":921,"user":{"displayName":"Kamy Nz","photoUrl":"","userId":"05812003528127999158"}}},"source":["def f1_checking_scraping_status(url):\n","\n","  \"\"\"Gets status of scraping status for a URL\n","\n","    Parameters\n","    ----------\n","    url : str\n","        The URL of the ramajudicial web page to be scraped\n","    Returns\n","    -------\n","    site_content: html\n","        Site content in .html to be scraped\n","    mssg: str\n","        A message saying that is ok to scrape or not\n","  \"\"\"\n","  http = urllib3.PoolManager()\n","  urlTemp = url\n","  resp = http.request('GET',urlTemp)\n","  site_content = resp.data.decode('utf-8')\n","\n","  \n","  if(resp.status==200):\n","    mssg = \"It is ok to do scraping\"\n","  else:\n","    mssg = \"We may need another idea\"\n","  return(site_content,mssg)\n","\n","def f2_writing_html_being_scraped(site_content,html_name, destination):\n","  \n","  \"\"\"Writes a html file from the webpag to be scraped\n","\n","    Parameters\n","    ----------\n","    site_content : str\n","        HTML object to be scraped with BeatifulSoup\n","    html_name : str\n","        Name of the HTML to be created\n","    destination : str\n","        PATH in which the created HTML is going to be saved\n","    Returns\n","    -------\n","    mssg : str\n","        A message saying that process went smoothly\n","  \"\"\"\n","  try:\n","      with open(destination+html_name,\"w\") as f:\n","        f.write(destination + site_content)\n","  except Exception as e:\n","    print(e)\n","\n","\n","def f3aux_generating_date_from_extracted_text(daytemp, monthtemp):\n","  #from unicodedata import normalize\n","  #normalize('NFKD', word)\n","  #import unicode\n","  from datetime import datetime\n","  from unicodedata import normalize\n","  \n","  # Making change in daytemp if apply\n","  #daytemp = normalize('NFKD',str(daytemp))\n","  daytemp = str(daytemp).strip()\n","  monthtemp = str(monthtemp).strip()\n","  yeartemp = str(datetime.today().year)\n","  #string_date = str(datetime.today().year)+\"-\"+str(month_number_temp).strip()+\"-\"+dayextracted\n","  date_string = yeartemp+\"/\"+monthtemp+\"/\"+daytemp\n","  date_transform_temp = datetime.strptime(date_string, \"%Y/%m/%d\")\n","  \n","  return(date_transform_temp,[daytemp,monthtemp,yeartemp])\n","  #dayextracted = str(atagitem.text).replace(u'\\xa0', u' ')\n","        #string_date = str(datetime.today().year)+\"-\"+str(month_number_temp).strip()+\"-\"+dayextracted\n","        #print(type(atagitem.text),month_number_temp.strip(),type(str(datetime.today().year)))\n","\n","def f3aux_normalizing_href(hrefinput):\n","\n","  href = hrefinput\n","\n","  if(href.startswith(\"/documents/\")):\n","    #print(hrefinput+\" To normalize point1\\n\")\n","    href_normalize = \"https://www.ramajudicial.gov.co\"+str(href)\n","  else:\n","    href_normalize = str(href)\n","  \n","  #print(href_normalize+\" To normalize point2\\n\")\n","  return(href_normalize)\n","        \n","\n","def f3_getting_positive_df_from_webpage(site_content,namesubjuzgado):\n","\n","  \"\"\"Gets the positive result of a change in status of law processes\n","\n","    Parameters\n","    ----------\n","    site_content: html\n","        Site content in .html to be scraped\n","    namesubjuzgado: str\n","        Name of subjuzgado to be inserted in dataframe to send email.\n","\n","    Returns\n","    -------\n","    df_final : pd.Dataframe\n","        Final dataframe with the dates in which href is present\n","  \"\"\"\n","\n","  from datetime import datetime\n","  import unicodedata\n","\n","  #clean_text = BeautifulSoup(raw_html, \"lxml\").text\n","  #print clean_text\n","  #u'Dear Parent,\\xa0This is a test message,\\xa0kindly ignore it.\\xa0Thanks'\n","  \n","  #new_str = unicodedata.normalize(\"NFKD\",clean_text)\n","  #print new_str\n","\n","  # Use html.parser to create soup\n","  s = BeautifulSoup(site_content1, 'lxml')\n","  #sclean = unicodedata.normalize(\"NFKD\",s)\n","\n","  search1 = s.find_all('div', {'class':'aui-tabview-content-item'})\n","  list_months = [\"Ene\",\"Feb\",\"Mar\",\n","  \"Abr\",\"May\",\"Jun\",\n","  \"Jul\",\"Ago\",\"Sep\",\n","  \"Oct\",\"Nov\",\"Dic\"]\n","  list_months_numbers = [\"01\",\"02\",\"03\",\n","  \"04\",\"05\",\"06\",\n","  \"07\",\"08\",\"09\",\n","  \"10\",\"11\",\"12\"]\n","  list_dfs_months = []\n","  count_months_with_table = 0\n","  namesubjuzgado = namesubjuzgado\n","\n","  for idx,item in enumerate(search1):\n","    if(item.find(['table'])):\n","      dfbymonth = pd.DataFrame(columns=('Tipo_Reporte','Nombre Juzgado','Mes','Hubo_Actualizaciones', 'Fecha_Actualizacion', 'PDF'))\n","      print(\"\\n Month\",list_months[idx],\"has table\")\n","      count_months_with_table +=1\n","      #print(item.prettify())\n","      month_name_temp = list_months[idx]\n","      month_number_temp = list_months_numbers[idx]\n","      searchTemp = item.find_all('a',href=True)\n","\n","      for idx,atagitem in enumerate(searchTemp):\n","      #print(\"True\",atagitem.text,atagitem['href'],atagitem.prettify())\n","      #print(\"True\",atagitem.text,atagitem['href'])\n","        #dayextracted = str(atagitem.text).replace(u'\\xa0', u' ')\n","        #string_date = str(datetime.today().year)+\"-\"+str(month_number_temp).strip()+\"-\"+dayextracted\n","        #print(type(atagitem.text),month_number_temp.strip(),type(str(datetime.today().year)))\n","        #date_temp = datetime.strptime(string_date, \"%Y-%m-%d\")\n","        hrefinput = atagitem['href']\n","        month_name_temp = str(month_name_temp)\n","        date_string,templist = f3aux_generating_date_from_extracted_text(atagitem.text,month_number_temp)\n","        href_normalize = f3aux_normalizing_href(hrefinput)\n","        dfbymonth.loc[idx] = ['Estados',namesubjuzgado,month_name_temp,\"Si\",date_string,href_normalize]\n","\n","      dfbymonth = dfbymonth.sort_values(by=\"Fecha_Actualizacion\",ascending=False)\n","      list_dfs_months.append(dfbymonth)\n","      #print(dfbymonth)\n","  #dfbymonth.dtypes\n","  #print(search1.prettify())\n","  print(\"\\nToday's date is\",datetime.now())\n","  #print(\"\\nThe number of months with\",count_months_with_table)\n","  df_final = pd.concat(list_dfs_months)\n","  return(df_final)\n","\n","def f4_writing_jsonfile(json_final,json_name,destination):\n","\n","  \"\"\"Gets the positive result of a change in status of law processes\n","\n","    Parameters\n","    ----------\n","    json_final : .json\n","        JSON object to be writing in a specific destination\n","    json_name : str\n","        Name of the JSON to be created\n","    destination : str\n","        PATH in which the created HTML is going to be saved\n","\n","    Returns\n","    -------\n","    mssg : str\n","        A message saying that process passed without issues\n","  \"\"\"\n","\n","  # Output as Json\n","  with open(destination+json_name,\"w\") as f:\n","    json.dump(destination+json_final,f)"],"execution_count":60,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BR_5wpj3aLKJ"},"source":["\n","\n","---\n","\n","\n","\n","---\n","\n"]},{"cell_type":"code","metadata":{"id":"ZxlTGmu7CR5G","executionInfo":{"status":"ok","timestamp":1616123659087,"user_tz":420,"elapsed":583,"user":{"displayName":"Kamy Nz","photoUrl":"","userId":"05812003528127999158"}}},"source":["def generating_merge_subjuzgados_report(dfbyreport,user_name,destination):\n","  # Import Drive API and authenticate.\n","  from google.colab import drive\n","  from datetime import date\n","  from datetime import datetime\n","\n","  drive.mount(\"/content/gdrive\",force_remount=True)\n","  root_dir = \"/content/gdrive/My Drive/\"\n","\n","  date_execution_report = str(date.today())\n","  csv_name = \"reporte_\"+user_name+\"_\"+date_execution_report\n","\n","  try:\n","    # Write the DataFrame to CSV file.\n","    with open(destination+csv_name+\".csv\", 'w') as f:\n","      dfbyreport.to_csv(f,index=False)\n","\n","  except Exception as e:\n","    print(e)\n","\n","  #df.to_csv('data.csv')\n","  #!cp data.csv \"drive/My Drive/\"\n","def structuring_sending_options():\n","  pass\n"],"execution_count":61,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F9b9X5JO_c5P"},"source":["## Setting up modules and PATHS to process data"]},{"cell_type":"code","metadata":{"id":"J_E0oqr3_TQe","executionInfo":{"status":"ok","timestamp":1616123660580,"user_tz":420,"elapsed":287,"user":{"displayName":"Kamy Nz","photoUrl":"","userId":"05812003528127999158"}}},"source":["# Libraries needed for basic web-scraping\n","from IPython.core.display import HTML\n","from bs4 import BeautifulSoup\n","from IPython.display import IFrame\n","import urllib3 # package required to interact with live webpage\n","import pandas as pd # will use to store the data from the webpage\n","import time\n","\n","# Steps to make:\n","# 1. Getting html from webpage with urllib \n","# 2. Save it in .html in order not to block the website for excessive scraping\n","# 3. Parse the .html with Beatiful Soupt\n","#!pip install -U -q PyDrive"],"execution_count":62,"outputs":[]},{"cell_type":"code","metadata":{"id":"2E86Kk1Ybx05","executionInfo":{"status":"ok","timestamp":1616123661278,"user_tz":420,"elapsed":265,"user":{"displayName":"Kamy Nz","photoUrl":"","userId":"05812003528127999158"}}},"source":["# Root Configurations\n","project_dir = \"project/datalab-colab-jupyter/\"\n","data_dir = \"project/data/\""],"execution_count":63,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZJq1S0V1PwOz","executionInfo":{"status":"ok","timestamp":1616123661728,"user_tz":420,"elapsed":285,"user":{"displayName":"Kamy Nz","photoUrl":"","userId":"05812003528127999158"}}},"source":["#import gspread\n","#from oauth2client.client import GoogleCredentials\n","\n","#gc = gspread.authorize(GoogleCredentials.get_application_default())\n","\n","#print(base_dir + data_dir+ 'input/db_juzgados')\n","#worksheet_sheet1 = gc.open(base_dir + data_dir + 'input/db_juzgados').worksheet(\"00ActiveOrganismoJudicial\")\n","#worksheet_sheet2 = gc.open(base_dir + data_dir + 'input/db_juzgados').sheet2"],"execution_count":64,"outputs":[]},{"cell_type":"code","metadata":{"id":"a3Sg-56Td_zl","executionInfo":{"status":"ok","timestamp":1616123662261,"user_tz":420,"elapsed":352,"user":{"displayName":"Kamy Nz","photoUrl":"","userId":"05812003528127999158"}}},"source":["#!pip install unicode"],"execution_count":65,"outputs":[]},{"cell_type":"code","metadata":{"id":"4Bqx56-AQYuT","executionInfo":{"status":"ok","timestamp":1616123662908,"user_tz":420,"elapsed":304,"user":{"displayName":"Kamy Nz","photoUrl":"","userId":"05812003528127999158"}}},"source":["URL = \"https://docs.google.com/spreadsheets/d/1MhVtLKsSr42jlesrTPa7nrN7Pn5wurxkjSKe77oiHvQ/edit?usp=sharing\""],"execution_count":66,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q2WRd9CsFZJT","executionInfo":{"status":"ok","timestamp":1616123664411,"user_tz":420,"elapsed":1269,"user":{"displayName":"Kamy Nz","photoUrl":"","userId":"05812003528127999158"}},"outputId":"f42d82cf-fec3-4d20-aa8b-604d276b31de"},"source":["df_act_juzgados, df_act_subjuzgados = f0_load_sheets_from_source(URL,'00ActiveOrganismoJudicial','01ClienteNora')\n","\n","#main_sheet = db_juzgados.worksheet('00ActiveOrganismoJudicial')\n","#juzgados_sheet = db_juzgados.worksheet('01ActiveJuzgados')"],"execution_count":67,"outputs":[{"output_type":"stream","text":["PuntoConsultaInicial                        object\n","PuntoConsultaSecundario                     object\n","DistritoJudicialAlQuePertenecePaginaWeb     object\n","LUGAR                                       object\n","NU_JUZGADOS                                float64\n","ACTIVE_MainJPMuni                          float64\n","dtype: object\n","\n","\n","Index(['PuntoConsultaInicial', 'PuntoConsultaSecundario',\n","       'DistritoJudicialAlQuePertenecePaginaWeb', 'LUGAR', 'NU_JUZGADOS',\n","       'ACTIVE_MainJPMuni'],\n","      dtype='object')\n","\n","\n","(999, 6)\n","\n","\n","<class 'pandas.core.frame.DataFrame'>\n","PuntoConsultaInicial                       float64\n","PuntoConsultaSecundario                    float64\n","DistritoJudicialAlQuePertenecePaginaWeb    float64\n","LUGAR                                       object\n","Nombre_JPMuni                               object\n","REPORTE                                     object\n","PUBLICAN                                    object\n","URL2                                        object\n","ANOTACIONES                                 object\n","SELECTED_SecondaryJPMuni                   float64\n","URL1                                        object\n","dtype: object\n","\n","\n","Index(['PuntoConsultaInicial', 'PuntoConsultaSecundario',\n","       'DistritoJudicialAlQuePertenecePaginaWeb', 'LUGAR', 'Nombre_JPMuni',\n","       'REPORTE', 'PUBLICAN', 'URL2', 'ANOTACIONES',\n","       'SELECTED_SecondaryJPMuni', 'URL1'],\n","      dtype='object')\n","\n","\n","(1000, 11)\n","\n","\n","<class 'pandas.core.frame.DataFrame'>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rQfR46bZhDly"},"source":["dfbymonth = pd.DataFrame(columns=('Tipo_Reporte','Nombre Juzgado','Mes',\n","                                  'Hubo_Actualizaciones', 'Fecha_Actualizacion', 'PDF'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mKbiEFuJfx2E"},"source":["## MAKING PROCESS FOR THIS SCENERAIO"]},{"cell_type":"code","metadata":{"id":"rcsMn3OJasOd","executionInfo":{"status":"ok","timestamp":1616123666459,"user_tz":420,"elapsed":292,"user":{"displayName":"Kamy Nz","photoUrl":"","userId":"05812003528127999158"}}},"source":["def f3_0_processing_excel_sheet(df_act_subjuzgados):\n","  df_check1 = df_act_subjuzgados[~df_act_subjuzgados[\"Nombre_JPMuni\"].isnull()]\n","  return(df_check1)"],"execution_count":68,"outputs":[]},{"cell_type":"code","metadata":{"id":"t0b7E3dXdU_O","executionInfo":{"status":"ok","timestamp":1616123667765,"user_tz":420,"elapsed":311,"user":{"displayName":"Kamy Nz","photoUrl":"","userId":"05812003528127999158"}}},"source":["df_check1 = f3_0_processing_excel_sheet(df_act_subjuzgados)\n"],"execution_count":69,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hODrnaQoavkM","executionInfo":{"status":"ok","timestamp":1616123644008,"user_tz":420,"elapsed":282,"user":{"displayName":"Kamy Nz","photoUrl":"","userId":"05812003528127999158"}},"outputId":"e8feec20-8f9d-4d63-d0fc-17c01109279e"},"source":["# Loop over functions\n","list_urlsubjuzgados = []\n","\n","for indexurl in range(3):\n","  name_tiporeporte = \"Estados\"\n","  namesubjuzgado = df_act_subjuzgados[\"Nombre_JPMuni\"].iloc[indexurl]\n","  urltemp = df_act_subjuzgados[\"URL2\"].iloc[indexurl]\n","  #print(urltemp)\n","\n","  "],"execution_count":57,"outputs":[{"output_type":"stream","text":["https://www.ramajudicial.gov.co/web/juzgado-01-civil-del-circuito-de-puerto-boyaca/80\n","https://www.ramajudicial.gov.co/web/juzgado-001-promiscuo-municipal-de-puerto-boyaca/71\n","https://www.ramajudicial.gov.co/web/juzgado-002-promiscuo-municipal-de-puerto-boyaca/71\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":504},"id":"hPYZHQqgYEwu","executionInfo":{"status":"error","timestamp":1616122500265,"user_tz":420,"elapsed":3262,"user":{"displayName":"Kamy Nz","photoUrl":"","userId":"05812003528127999158"}},"outputId":"9fc6ee5b-fdeb-4ea7-ed8a-687b2a100503"},"source":["# Loop over functions\n","list_urlsubjuzgados = []\n","for indexurl in range(3):\n","  name_tiporeporte = \"Estados\"\n","  namesubjuzgado = df_act_subjuzgados[\"Nombre_JPMuni\"].iloc[indexurl]\n","  urltemp = df_act_subjuzgados[\"URL2\"].iloc[indexurl]\n","\n","  if(urltemp != None ):\n","    print(\"\\n Se esta corriendo para\",indexurl, namesubjuzgado)\n","    #if(indexurl != 1 and indexurl !=4):\n","    site_content1, mssg1 = f1_checking_scraping_status(urltemp)\n","    #f2_writing_html_being_scraped(urltemp,\"jcivilmuni_honda1.html\",base_dir+data_dir)\n","    #df_temp = f3_getting_positive_df_from_webpage(site_content1,namesubjuzgado)\n","    #print(df_temp)\n","    #list_urlsubjuzgados.append(df_temp)\n","\n","#df_final_urlsubjuzgados = pd.concat(list_urlsubjuzgados)"],"execution_count":38,"outputs":[{"output_type":"stream","text":["\n"," Se esta corriendo para 0 JUZGADO 01 CIVIL DEL CIRCUITO DE PUERTO BOYACA\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n","  InsecureRequestWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["\n"," Month Ene has table\n"],"name":"stdout"},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-38-96b9f2c50bf0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0msite_content1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmssg1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_checking_scraping_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murltemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m#f2_writing_html_being_scraped(urltemp,\"jcivilmuni_honda1.html\",base_dir+data_dir)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mdf_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf3_getting_positive_df_from_webpage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msite_content1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnamesubjuzgado\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_temp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mlist_urlsubjuzgados\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_temp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-37-234a7e7a738a>\u001b[0m in \u001b[0;36mf3_getting_positive_df_from_webpage\u001b[0;34m(site_content, namesubjuzgado)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mhrefinput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0matagitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'href'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mmonth_name_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonth_name_temp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mdate_string\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtemplist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf3aux_generating_date_from_extracted_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matagitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmonth_number_temp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0mhref_normalize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf3aux_normalizing_href\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhrefinput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mdfbymonth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Estados'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnamesubjuzgado\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmonth_name_temp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Si\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdate_string\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhref_normalize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-37-234a7e7a738a>\u001b[0m in \u001b[0;36mf3aux_generating_date_from_extracted_text\u001b[0;34m(daytemp, monthtemp)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;31m#string_date = str(datetime.today().year)+\"-\"+str(month_number_temp).strip()+\"-\"+dayextracted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m   \u001b[0mdate_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myeartemp\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmonthtemp\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdaytemp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m   \u001b[0mdate_transform_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"%Y/%m/%d\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m   \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate_transform_temp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdaytemp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmonthtemp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myeartemp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/_strptime.py\u001b[0m in \u001b[0;36m_strptime_datetime\u001b[0;34m(cls, data_string, format)\u001b[0m\n\u001b[1;32m    575\u001b[0m     \"\"\"Return a class cls instance based on the input string and the\n\u001b[1;32m    576\u001b[0m     format string.\"\"\"\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mtt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfraction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgmtoff_fraction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_strptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m     \u001b[0mtzname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgmtoff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfraction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/_strptime.py\u001b[0m in \u001b[0;36m_strptime\u001b[0;34m(data_string, format)\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         raise ValueError(\"unconverted data remains: %s\" %\n\u001b[0;32m--> 362\u001b[0;31m                           data_string[found.end():])\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0miso_year\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: unconverted data remains: 18-00033"]}]},{"cell_type":"code","metadata":{"id":"-cDT95j8euad"},"source":["def f3_getting_positive_df_from_webpage_with_scenarios(site_content,namesubjuzgado,numberscenario):\n","\n","  \"\"\"Gets the positive result of a change in status of law processes\n","\n","    Parameters\n","    ----------\n","    site_content: html\n","        Site content in .html to be scraped\n","    namesubjuzgado: str\n","        Name of subjuzgado to be inserted in dataframe to send email.\n","\n","    Returns\n","    -------\n","    df_final : pd.Dataframe\n","        Final dataframe with the dates in which href is present\n","  \"\"\"\n","\n","  from datetime import datetime\n","  import unicodedata\n","\n","  #clean_text = BeautifulSoup(raw_html, \"lxml\").text\n","  #print clean_text\n","  #u'Dear Parent,\\xa0This is a test message,\\xa0kindly ignore it.\\xa0Thanks'\n","  \n","  #new_str = unicodedata.normalize(\"NFKD\",clean_text)\n","  #print new_str\n","\n","  if(numberscenario == 0):\n","    # Use html.parser to create soup\n","    s = BeautifulSoup(site_content1, 'lxml')\n","    #sclean = unicodedata.normalize(\"NFKD\",s)\n","\n","    search1 = s.find_all('div', {'class':'aui-tabview-content-item'})\n","    list_months = [\"Ene\",\"Feb\",\"Mar\",\n","    \"Abr\",\"May\",\"Jun\",\n","    \"Jul\",\"Ago\",\"Sep\",\n","    \"Oct\",\"Nov\",\"Dic\"]\n","    list_months_numbers = [\"01\",\"02\",\"03\",\n","    \"04\",\"05\",\"06\",\n","    \"07\",\"08\",\"09\",\n","    \"10\",\"11\",\"12\"]\n","    list_dfs_months = []\n","    count_months_with_table = 0\n","    namesubjuzgado = namesubjuzgado\n","\n","    for idx,item in enumerate(search1):\n","      if(item.find(['table'])):\n","        dfbymonth = pd.DataFrame(columns=('Tipo_Reporte','Nombre Juzgado','Mes','Hubo_Actualizaciones', 'Fecha_Actualizacion', 'PDF'))\n","        print(\"\\n Month\",list_months[idx],\"has table\")\n","        count_months_with_table +=1\n","        #print(item.prettify())\n","        month_name_temp = list_months[idx]\n","        month_number_temp = list_months_numbers[idx]\n","        searchTemp = item.find_all('a',href=True)\n","\n","        for idx,atagitem in enumerate(searchTemp):\n","        #print(\"True\",atagitem.text,atagitem['href'],atagitem.prettify())\n","        #print(\"True\",atagitem.text,atagitem['href'])\n","          #dayextracted = str(atagitem.text).replace(u'\\xa0', u' ')\n","          #string_date = str(datetime.today().year)+\"-\"+str(month_number_temp).strip()+\"-\"+dayextracted\n","          #print(type(atagitem.text),month_number_temp.strip(),type(str(datetime.today().year)))\n","          #date_temp = datetime.strptime(string_date, \"%Y-%m-%d\")\n","          hrefinput = atagitem['href']\n","          month_name_temp = str(month_name_temp)\n","          date_string,templist = f3aux_generating_date_from_extracted_text(atagitem.text,month_number_temp)\n","          href_normalize = f3aux_normalizing_href(hrefinput)\n","          dfbymonth.loc[idx] = ['Estados',namesubjuzgado,month_name_temp,\"Si\",date_string,href_normalize]\n","\n","        dfbymonth = dfbymonth.sort_values(by=\"Fecha_Actualizacion\",ascending=False)\n","        list_dfs_months.append(dfbymonth)\n","        #print(dfbymonth)\n","    #dfbymonth.dtypes\n","    #print(search1.prettify())\n","    print(\"\\nToday's date is\",datetime.now())\n","    #print(\"\\nThe number of months with\",count_months_with_table)\n","    df_final = pd.concat(list_dfs_months)\n","  if(numberscenario == 1):\n","    \n","\n","\n","\n","  return(df_final)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aiWu1snPgO9p","executionInfo":{"status":"aborted","timestamp":1616122500152,"user_tz":420,"elapsed":3138,"user":{"displayName":"Kamy Nz","photoUrl":"","userId":"05812003528127999158"}}},"source":["df_final_urlsubjuzgados"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cv0jBBLIHVhg","executionInfo":{"status":"aborted","timestamp":1616122500157,"user_tz":420,"elapsed":3138,"user":{"displayName":"Kamy Nz","photoUrl":"","userId":"05812003528127999158"}}},"source":["#df_final_urlsubjuzgados.to_csv()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TH1G03HMYrPp","executionInfo":{"status":"aborted","timestamp":1616122500159,"user_tz":420,"elapsed":3128,"user":{"displayName":"Kamy Nz","photoUrl":"","userId":"05812003528127999158"}}},"source":["#P1:Validaciones bajo la muestra de juzgados resaltados (Tipo Reportes => Estados (Para hoy enfocados en Estados) y Traslados (Manana se valida con Traslados)):\n","# -Se entendio que toca hacer escenarios para pagina de estados de Juzgados dentro del conjunto de muestras en serie 2\n","#P2:Generar dataframe con nombre de juzgados y tipo de reporte,ya\n","#P3:Si tiene https => no agregar , si no agregar prefijo de ramajudicial,ya\n","#P4: Falta agregarle al archivo de .csv el timestamp de hora ejecutado\n","#P5:Enviar correo con Archivo adjunto,falta\n","\n","\n","#-Funcion para detectar documentos de SharePoint, proxima semana\n","#-Pipeline para obtener metadata de archivo PDF(tiporeporte-juzgado-dia), proxima semana\n","user_name = \"oficina_AVT\"\n","#date_execution_report = \n","generating_merge_subjuzgados_report(df_final_urlsubjuzgados,user_name,base_dir+data_dir+\"output_report_by_client/\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zgxdo7NRCkQa"},"source":["## Generating .csv after getting last concat of dataframe to email"]},{"cell_type":"code","metadata":{"id":"aqqw9a-sNLZE","executionInfo":{"status":"aborted","timestamp":1616122500162,"user_tz":420,"elapsed":3110,"user":{"displayName":"Kamy Nz","photoUrl":"","userId":"05812003528127999158"}}},"source":["# Loading sheets from GoogleSheets in GoogleDrive\n","def f0_load_sheets_from_source(source_url,namesheet1,namesheet2):\n","  \"\"\"Gets the positive result of a change in status of law processes\n","\n","    Parameters\n","    ----------\n","    source_url : url address\n","        Link to GoogleSheets to parse as db_juzgados\n","    json_name : str\n","        Name of the JSON to be created\n","    destination : str\n","        PATH in which the created HTML is going to be saved\n","\n","    Returns\n","    -------\n","    df_act_juzgados : str\n","        A message saying that process passed without issues\n","\n","    df_act_subjuzgados : str\n","        A message saying that process passed without issues\n","\n","    prints : Different prints\n","        Differents prints of the processing for loading sheets\n","  \"\"\"\n","  db_juzgados = gc.open_by_url(source_url)\n","  main_sheet = db_juzgados.worksheet(namesheet1)\n","  juzgados_sheet = db_juzgados.worksheet(namesheet2)\n","  df_act_juzgados = get_as_dataframe(main_sheet)\n","  df_act_juzgados = df_act_juzgados.loc[:,~df_act_juzgados.columns.str.match(\"Unnamed\")]\n","\n","  df_act_subjuzgados = get_as_dataframe(juzgados_sheet)\n","  df_act_subjuzgados = df_act_subjuzgados.loc[:,~df_act_subjuzgados.columns.str.match(\"Unnamed\")]\n","\n","  #dict_df_query = {}\n","  #for tab in query_list:\n","  #  dict_df_query[tab] = pd.read_excel(io.BytesIO(io_query),sheet_name=tab)\n","  #return(dict_df_query)\n","\n","  print(df_act_juzgados.dtypes)\n","  print(\"\\n\")\n","  print(df_act_juzgados.columns)\n","  print(\"\\n\")\n","  print(df_act_juzgados.shape)\n","  print(\"\\n\")\n","  print(type(df_act_juzgados))\n","\n","  print(df_act_subjuzgados.dtypes)\n","  print(\"\\n\")\n","  print(df_act_subjuzgados.columns)\n","  print(\"\\n\")\n","  print(df_act_subjuzgados.shape)\n","  print(\"\\n\")\n","  print(type(df_act_subjuzgados))\n","\n","  return(df_act_juzgados,df_act_subjuzgados)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rkxLs3h5f5bw","executionInfo":{"status":"aborted","timestamp":1616122500163,"user_tz":420,"elapsed":3101,"user":{"displayName":"Kamy Nz","photoUrl":"","userId":"05812003528127999158"}}},"source":["def f1_checking_scraping_status(url):\n","\n","  \"\"\"Gets status of scraping status for a URL\n","\n","    Parameters\n","    ----------\n","    url : str\n","        The URL of the ramajudicial web page to be scraped\n","    Returns\n","    -------\n","    site_content: html\n","        Site content in .html to be scraped\n","    mssg: str\n","        A message saying that is ok to scrape or not\n","  \"\"\"\n","  http = urllib3.PoolManager()\n","  urlTemp = url\n","  resp = http.request('GET',urlTemp)\n","  site_content = resp.data.decode('utf-8')\n","\n","  \n","  if(resp.status==200):\n","    mssg = \"It is ok to do scraping\"\n","  else:\n","    mssg = \"We may need another idea\"\n","  return(site_content,mssg)\n","\n","def f2_writing_html_being_scraped(site_content,html_name, destination):\n","  \n","  \"\"\"Writes a html file from the webpag to be scraped\n","\n","    Parameters\n","    ----------\n","    site_content : str\n","        HTML object to be scraped with BeatifulSoup\n","    html_name : str\n","        Name of the HTML to be created\n","    destination : str\n","        PATH in which the created HTML is going to be saved\n","    Returns\n","    -------\n","    mssg : str\n","        A message saying that process went smoothly\n","  \"\"\"\n","  try:\n","      with open(destination+html_name,\"w\") as f:\n","        f.write(destination + site_content)\n","  except Exception as e:\n","    print(e)\n","\n","\n","def f3aux_generating_date_from_extracted_text(daytemp, monthtemp):\n","  #from unicodedata import normalize\n","  #normalize('NFKD', word)\n","  #import unicode\n","  from datetime import datetime\n","  from unicodedata import normalize\n","  \n","  # Making change in daytemp if apply\n","  #daytemp = normalize('NFKD',str(daytemp))\n","  daytemp = str(daytemp).strip()\n","  monthtemp = str(monthtemp).strip()\n","  yeartemp = str(datetime.today().year)\n","  #string_date = str(datetime.today().year)+\"-\"+str(month_number_temp).strip()+\"-\"+dayextracted\n","  date_string = yeartemp+\"/\"+monthtemp+\"/\"+daytemp\n","  date_transform_temp = datetime.strptime(date_string, \"%Y/%m/%d\")\n","  \n","  return(date_transform_temp,[daytemp,monthtemp,yeartemp])\n","  #dayextracted = str(atagitem.text).replace(u'\\xa0', u' ')\n","        #string_date = str(datetime.today().year)+\"-\"+str(month_number_temp).strip()+\"-\"+dayextracted\n","        #print(type(atagitem.text),month_number_temp.strip(),type(str(datetime.today().year)))\n","\n","def f3aux_normalizing_href(hrefinput):\n","\n","  href = hrefinput\n","\n","  if(href.startswith(\"/documents/\")):\n","    #print(hrefinput+\" To normalize point1\\n\")\n","    href_normalize = \"https://www.ramajudicial.gov.co\"+str(href)\n","  else:\n","    href_normalize = str(href)\n","  \n","  #print(href_normalize+\" To normalize point2\\n\")\n","  return(href_normalize)\n","        \n","\n","def f3_getting_positive_df_from_webpage(site_content,namesubjuzgado):\n","\n","  \"\"\"Gets the positive result of a change in status of law processes\n","\n","    Parameters\n","    ----------\n","    site_content: html\n","        Site content in .html to be scraped\n","    namesubjuzgado: str\n","        Name of subjuzgado to be inserted in dataframe to send email.\n","\n","    Returns\n","    -------\n","    df_final : pd.Dataframe\n","        Final dataframe with the dates in which href is present\n","  \"\"\"\n","\n","  from datetime import datetime\n","  import unicodedata\n","\n","  #clean_text = BeautifulSoup(raw_html, \"lxml\").text\n","  #print clean_text\n","  #u'Dear Parent,\\xa0This is a test message,\\xa0kindly ignore it.\\xa0Thanks'\n","  \n","  #new_str = unicodedata.normalize(\"NFKD\",clean_text)\n","  #print new_str\n","\n","  # Use html.parser to create soup\n","  s = BeautifulSoup(site_content1, 'lxml')\n","  #sclean = unicodedata.normalize(\"NFKD\",s)\n","\n","  search1 = s.find_all('div', {'class':'aui-tabview-content-item'})\n","  list_months = [\"Ene\",\"Feb\",\"Mar\",\n","  \"Abr\",\"May\",\"Jun\",\n","  \"Jul\",\"Ago\",\"Sep\",\n","  \"Oct\",\"Nov\",\"Dic\"]\n","  list_months_numbers = [\"01\",\"02\",\"03\",\n","  \"04\",\"05\",\"06\",\n","  \"07\",\"08\",\"09\",\n","  \"10\",\"11\",\"12\"]\n","  list_dfs_months = []\n","  count_months_with_table = 0\n","  namesubjuzgado = namesubjuzgado\n","\n","  for idx,item in enumerate(search1):\n","    if(item.find(['table'])):\n","      dfbymonth = pd.DataFrame(columns=('Tipo_Reporte','Nombre Juzgado','Mes','Hubo_Actualizaciones', 'Fecha_Actualizacion', 'PDF'))\n","      print(\"\\n Month\",list_months[idx],\"has table\")\n","      count_months_with_table +=1\n","      #print(item.prettify())\n","      month_name_temp = list_months[idx]\n","      month_number_temp = list_months_numbers[idx]\n","      searchTemp = item.find_all('a',href=True)\n","\n","      for idx,atagitem in enumerate(searchTemp):\n","      #print(\"True\",atagitem.text,atagitem['href'],atagitem.prettify())\n","      #print(\"True\",atagitem.text,atagitem['href'])\n","        #dayextracted = str(atagitem.text).replace(u'\\xa0', u' ')\n","        #string_date = str(datetime.today().year)+\"-\"+str(month_number_temp).strip()+\"-\"+dayextracted\n","        #print(type(atagitem.text),month_number_temp.strip(),type(str(datetime.today().year)))\n","        #date_temp = datetime.strptime(string_date, \"%Y-%m-%d\")\n","        hrefinput = atagitem['href']\n","        month_name_temp = str(month_name_temp)\n","        date_string,templist = f3aux_generating_date_from_extracted_text(atagitem.text,month_number_temp)\n","        href_normalize = f3aux_normalizing_href(hrefinput)\n","        dfbymonth.loc[idx] = ['Estados',namesubjuzgado,month_name_temp,\"Si\",date_string,href_normalize]\n","\n","      dfbymonth = dfbymonth.sort_values(by=\"Fecha_Actualizacion\",ascending=False)\n","      list_dfs_months.append(dfbymonth)\n","      #print(dfbymonth)\n","  #dfbymonth.dtypes\n","  #print(search1.prettify())\n","  print(\"\\nToday's date is\",datetime.now())\n","  #print(\"\\nThe number of months with\",count_months_with_table)\n","  df_final = pd.concat(list_dfs_months)\n","  return(df_final)\n","\n","def f4_writing_jsonfile(json_final,json_name,destination):\n","\n","  \"\"\"Gets the positive result of a change in status of law processes\n","\n","    Parameters\n","    ----------\n","    json_final : .json\n","        JSON object to be writing in a specific destination\n","    json_name : str\n","        Name of the JSON to be created\n","    destination : str\n","        PATH in which the created HTML is going to be saved\n","\n","    Returns\n","    -------\n","    mssg : str\n","        A message saying that process passed without issues\n","  \"\"\"\n","\n","  # Output as Json\n","  with open(destination+json_name,\"w\") as f:\n","    json.dump(destination+json_final,f)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BrB55pPcN8RR"},"source":["\n","\n","---\n","\n"]},{"cell_type":"code","metadata":{"id":"gg9mh8rYM7tN","executionInfo":{"status":"aborted","timestamp":1616122500164,"user_tz":420,"elapsed":3100,"user":{"displayName":"Kamy Nz","photoUrl":"","userId":"05812003528127999158"}}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UWojh2JMNLhl","executionInfo":{"status":"aborted","timestamp":1616122500165,"user_tz":420,"elapsed":3097,"user":{"displayName":"Kamy Nz","photoUrl":"","userId":"05812003528127999158"}}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9-h6u_EdNLlv","executionInfo":{"status":"aborted","timestamp":1616122500167,"user_tz":420,"elapsed":3096,"user":{"displayName":"Kamy Nz","photoUrl":"","userId":"05812003528127999158"}}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MOghrUo_M78W","executionInfo":{"status":"aborted","timestamp":1616122500264,"user_tz":420,"elapsed":3191,"user":{"displayName":"Kamy Nz","photoUrl":"","userId":"05812003528127999158"}}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t0XPUla8M8gM"},"source":["## Questions to take into account:\n","\n","### What is \"data\"?\n","\n","You can go onto Wikipedia or read books to get an answer to this question, but most of those sources will give you a very pedantic, unintuitive definition. Instead, we're going to go with the colloquial definition of data as **\"something whose value you care about”**. You won't find that in any formal treatment of the subject, but for now, it is good enough. Your name, age, and telephone number are data about you. Your bank savings, your address, and your parents' names are data that relate to you. We have data about everything, everywhere.\n","\n","## What is data science? (10 mts)\n","\n","Now that we know what data is, we can now ask: \"What is data science?\" Science, in the language of the scientific method, is:\n","\n","1. Formulating hypotheses, or guesses about how the world works, based on observations of the world around us\n","2. Validating or invalidating those hypotheses by conducting experiments\n","\n","Unlike the pure sciences though, working with data doesn't necessarily require conducting experiments (although it could!). Rather, many times the data has already been collected and organized by someone else. So the scientific method, as applied to data, can be summarized as: **\"Formulating hypotheses based on the world around us, then analyzing relevant data to validate or invalidate our hypotheses.\"**\n","\n","## What is machine learning?\n","\n","However, choice (d) of Exercise 1, as well as examples 1 and 2 from the above list, do fall into the bucket of **machine learning**. What does this mean?\n","\n","\"Learn\" means to \"gain or acquire knowledge or skill in something via experience.\" So one could frame \"machine learning\" as \"how a machine gains or acquires knowledge via experience.\" How does a machine gain experience? All machine inputs are essentially binary strings of 0s and 1s, which is really just – you guessed it – data! So machine learning is really just **how a computer acquires knowledge via data**.\n","\n","Of course, this gives no insight into the \"how\" at all; it just says that there is soemething that is done with input data to generate this knowledge as an output. To make a math analogy, machine learning is some function $f$ such that\n","\n","> $ knowledge = f(data)$\n","\n","and other than that there are no other real stipulations on $f$! So $f$ could be as mechanical as a simple mathematical function (say, the sum of all the data points) and qualify as machine learning. And in practice, this is what most of the common machine learning algorithms are, including:\n","\n","1. Logistic regression\n","2. Random forests\n","3. Support vector machines\n","4. $k$ - means clustering\n","5. Neural networks\n","\n","(You will learn about all of these later in the course.) This may seem disappointing, given how the media hypes up \"artificial intelligence\" and makes it seem like there is something \"smart\" going on with machine learning, but in fact many mechanical methods satisfy the conditions required to be classified as machine learning. This doesn't mean these mechanical methods are limited in usefulness – in fact, they are quite powerful if used properly – but it does mean that they don't resemble anything that we would naturally associate with human-like intelligence.\n","\n","## What is artificial intelligence? \n","\n","But the elephant is still in the room: even though some mechanical, \"dumb\" methods may qualify as machine learning, this doesn't exclude human-like, \"smart\" methods from being classified as such either. And semantically, this is completely true – it doesn't, yet people have chosen to name it something else entirely: **artificial intelligence**.\n","\n","But why? Why give \"smart\" methods an entirely different name if they can also fall under the bucket of machine learning? That is the question we will explore for the remainder of this module.\n","\n","Let's start by taking a look at an iconic demonstration of this so-called intelligence: AlphaGo beating the world's top human Go player.\n","\n","### What can machines do and not do? What about us? \n","\n","So, is there any sensible test we could use to determine if something is as intelligent as a human? There have been many proposals over time. The most famous aptitude test developed was the **Turing test**, named after the English mathematician and famous World War II cryptographer Alan Turing. In this exam, there is a human evaluator and two conversation partners: one machine and one human. The evaluator would conduct a conversation with each through a text-only channel. If the evaluator cannot reliably tell the machine from the human, the machine is said to have passed the test.\n","\n","Turing did not explicitly state that his test could be used as a measure of intelligence, but nonetheless many who came after him thrust his test into the limelight. Of course, the corollary of this is that if a computer can converse like a human, then it is effectively as intelligent as a human.\n","\n","## Conclusions & Takeaways\n","\n","In this discussion, you learned what \"data science\" and \"machine learning\" really are, in contrast to the misleading connotations that they are often given in public discussion. You also learned that \"artificial intelligence\" is a very murky term – nobody really knows what it is, and it's unclear if in its current focus on imitating human intelligence, that it is even a good use of our time and effort.\n","\n","Throughout this program, we will focus primarily on data science and machine learning, not so much artificial intelligence. Yet the philosophical questions surrounding artificial intelligence are fascinating, and we encourage you to continue pondering them as you become more involved in this new and exciting field."]}]}